# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** karansalunkhe21
**Raw Score:** 47/50 (94.0%)
**Course Points Earned:** 4

---

## Problem Breakdown

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Well done. You correctly apply t-SNE to MNIST, sensibly use a subset for speed, set perplexity and random_state, and produce a clear scatter plot with labels. This meets the exercise goal.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you reduced to 2D with t-SNE and trained KNN, reporting accuracy on a validation split. That directly answers performance. Minor: you compute a PCA transform for a test set you don’t use; also note t-SNE isn’t parametric for test transforms.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job. You correctly used UMAP to embed the data, trained KNN on the UMAP space, and computed validation accuracy. Train/val split is appropriate and the workflow is sound. Full credit.

---

### Exercise 4 (17/20 = 85.0%)

**Part ex2-part1** (ex2-part1.code): 7/7 points

_Feedback:_ Good job: you applied PCA (80% variance) and trained/evaluated KNN correctly. Using 0.80 vs 0.90 variance is acceptable. Consider adding a 2D scatter of first two PCs for inspection as in the reference. The second cell is empty but main task is achieved.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Well done. You correctly applied UMAP on training data, transformed test, trained KNN, and reported accuracy. Parameters are reasonable and consistent with prior work. Optional: add a 2D scatter of X_train_umap colored by y_train for insight.

**Part ex2-part3** (ex2-part3.answer): 3/6 points

_Feedback:_ You correctly compared PCA vs UMAP, reported accuracies, and gave a solid reason why UMAP can perform better. For higher credit, discuss parameter exploration (e.g., varying n_neighbors/min_dist) and note that UMAP often shines in low dims with low n_neighbors.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Well done. You reduced to 2D with PCA and produced a clear scatter plot colored by class with a colorbar and labels. This fulfills the task requirements. Minor optional: tweak point size (s) for dense plots to improve visibility.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Excellent scree plot. You correctly fit PCA with 40 components, computed percent variance explained, and plotted components 1–40. Labels and presentation meet the requirements.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach: fit PCA on full data, compute cumulative explained variance, and find minimal components to reach 95%. Uses prior data appropriately. Clear and correct.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Excellent. You correctly used n_components_95 from prior work, reduced the first digit, inverse-transformed it, and visualized the reconstruction with the provided function. This meets the task’s goal. Minor redundancy: plot_mnist_digit already calls plt.show().

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Good job: you compared KNN with and without PCA and preserved 80% variance using PCA(0.80). You reported accuracies and also computed the component count from prior cumulative variance. Solid, correct implementation aligned with prior work.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-11-11 17:33:47 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*