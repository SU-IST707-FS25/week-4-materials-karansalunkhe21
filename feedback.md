# Assignment Feedback: Week 4: Dimensionality Reduction

**Student:** karansalunkhe21
**Total Score:** 28/40 (70.0%)

**Grade Category:** C- (Satisfactory)

---

## Problem Breakdown

### Exercise 1 (8/16 = 50.0%)

**Part pipeline-part1** (pipeline-part1.code): 0/0 points

_Feedback:_ You correctly applied PCA to 2D on the training set and visualized the classes with a scatter plot—this meets the core goal. For a fuller approximation view, you could also project the test set and/or show reconstructed images via inverse_transform. Nicely done.

**Part pipeline-part2** (pipeline-part2.code): 2/4 points

_Feedback:_ You correctly fit PCA and produced a scree plot, but the task required reducing to 2 components and visualizing a 2D scatter colored by class. No 2D transform or class-colored scatter is shown. Use PCA(n_components=2), transform, and scatter with c=y_mnist_train.

**Part pipeline-part3** (pipeline-part3.code): 1/4 points

_Feedback:_ You did PCA and computed cumulative explained variance, but the task required a scree plot for the first 40 components with y-axis as percent variance explained. No plot was produced, not limited to first 40, and axis labeling not as specified. Add the plot as in your prior part2

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Excellent. You correctly used the previously computed n_components_95 to fit PCA and demonstrated its effect via reconstruction. This meets the goal of selecting components for 95% variance. Nice added reconstruction/visualization.

**Part pipeline-part5** (pipeline-part5.code): 1/4 points

_Feedback:_ You applied PCA and KNN, but the task was to visualize a digit reconstructed from the reduced space using the dimensions from Step 4 (n_components_95). No digit visualization or reconstruction is done here, and you used 80% variance instead. Provide inverse_transform and plot.

---

### Exercise 2 (10/10 = 100.0%)

**Part ex1-part1** (ex1-part1.code): 4/4 points

_Feedback:_ Good job. You correctly applied t-SNE to MNIST (using a reasonable 2k subset), colored by labels, and produced a clear 2D visualization with a legend. This meets the goal. Minor optional tweaks: adjust point size for clarity or experiment with perplexity.

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you fit t-SNE on the train subset, split into train/val in the 2D space, trained KNN, and reported accuracy—this answers performance. Minor: t-SNE isn’t parametric, so your held-out validation within the same t-SNE fit is fine; the unused X_test_* can be removed.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Good job. You fit UMAP on the training split, transformed validation, trained KNN, and reported accuracy. This cleanly computes KNN accuracy on UMAP embeddings and respects train/val separation. Full credit.

---

### Exercise 4 (10/14 = 71.4%)

**Part ex2-part1** (ex2-part1.code): 0/0 points

_Feedback:_ You correctly applied PCA (80% variance) and evaluated KNN. However, you didn’t vary dimensionality, didn’t try UMAP or tune its parameters, and didn’t include visualizations. One cell is an empty placeholder. Please add PCA/UMAP comparisons, parameter sweeps, and plots.

**Part ex2-part2** (ex2-part2.code): 3/7 points

_Feedback:_ You implemented UMAP + KNN, but the task was to try with PCA and to build on your prior PCA work. Please apply PCA (e.g., PCA(0.80)), transform train/test, train KNN, and report accuracy. The UMAP code itself is fine. The second cell is just a placeholder.

**Part ex2-part3** (ex2-part3.answer): 7/7 points

_Feedback:_ Clear comparison using your reported results and a sound explanation of why UMAP can outperform PCA (nonlinear manifold vs linear variance directions). Good interpretation aligned with your prior work. Well done.

---

## Additional Information

This feedback was automatically generated by the autograder using LLM-based evaluation.

**Generated:** 2025-10-27 18:51:19 UTC

If you have questions about your grade, please reach out to the instructor.

---

*Powered by [Grade-Lite](https://github.com/your-repo/grade-lite) Autograder*